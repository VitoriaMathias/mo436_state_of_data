{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vi_ma\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from imblearn.over_sampling import ADASYN, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "train = pd.read_csv(\"../data/train_complete.csv\")\n",
    "test = pd.read_csv(\"../data/test_complete.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataprep - toda transformação deve ser criada sobre o teste e somente aplicada sobre o treino\n",
    "def apply_random_oversampling(df, oversample_col='', exclude_category=None):\n",
    "    '''\n",
    "    df: dataframe with all columns\n",
    "    oversample_col: column to apply the oversampling\n",
    "    exclude_category: list of categories on oversample_col to set apart while oversampling\n",
    "    '''\n",
    "    df_cat = pd.DataFrame(columns = df.columns)\n",
    "    df_ = df.copy()\n",
    "    \n",
    "    # Separando categorias específica se necessário\n",
    "    if exclude_category is not None:\n",
    "        \n",
    "        for cat in exclude_category:\n",
    "            \n",
    "            df_cat = pd.concat([df_cat, df[df[oversample_col]==cat]]).reset_index(drop=True)\n",
    "            \n",
    "            # removendo categorias que não serão oversampled\n",
    "            df_ = df_[df[oversample_col]!=cat].reset_index()\n",
    "    \n",
    "    # Separando coluna de oversampling\n",
    "    X = df_.drop(columns=[oversample_col])\n",
    "    y = df_[oversample_col]\n",
    "\n",
    "    # Aplicando Random Oversampling\n",
    "    oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "    df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    df_resampled[oversample_col] = y_resampled\n",
    "\n",
    "    # Exibir o DataFrame balanceado\n",
    "    print(df_resampled[oversample_col].value_counts())\n",
    "    \n",
    "    # inserindo de volta as categorias que foram separadas\n",
    "    if not df_cat.empty:\n",
    "        df_resampled = pd.concat([df_resampled, df_cat]).reset_index(drop=True)\n",
    "        \n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_random_undersampling(df, undersample_col='', strategy={}):\n",
    "    '''\n",
    "    df: dataframe with all columns\n",
    "    undersample_col: column to apply the undersampling\n",
    "    srtategy: dictionary with the category as key and number of samples requested as value\n",
    "    '''\n",
    "\n",
    "    X = df.drop(columns=[undersample_col])\n",
    "    y = df[undersample_col]\n",
    "\n",
    "    # aplicando undersampling\n",
    "    undersampler = RandomUnderSampler(sampling_strategy=strategy, random_state=42)\n",
    "    X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "\n",
    "    df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    df_resampled[undersample_col] = y_resampled\n",
    "\n",
    "    # Exibir o DataFrame balanceado e a distribuição de classes\n",
    "    print(df_resampled[undersample_col].value_counts())\n",
    "    \n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['P0', 'P1_a', 'P1_a_1', 'P1_b', 'P1_c', 'P1_d', 'P1_e', 'P1_e_1',\n",
       "       'P1_e_2', 'P1_e_3',\n",
       "       ...\n",
       "       'P6_g_Presto(OneHot)', 'P6_g_Snowflake(OneHot)',\n",
       "       'P6_g_Teradata(OneHot)', 'P6_g_nan(OneHot)', 'P1_l(OrdEnc)',\n",
       "       'P2_e(OrdEnc)', 'P2_g(OrdEnc)', 'P2_i(OrdEnc)', 'P2_j(OrdEnc)',\n",
       "       'P2_h(OrdEnc)'],\n",
       "      dtype='object', length=724)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vi_ma\\AppData\\Local\\Temp\\ipykernel_5144\\4237983600.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_resampled[oversample_col] = y_resampled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branca                  2452\n",
      "Preta                   2452\n",
      "Parda                   2452\n",
      "Amarela                 2452\n",
      "Outra                   2452\n",
      "Prefiro não informar    2452\n",
      "Indígena                2452\n",
      "Name: P1_c, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vi_ma\\AppData\\Local\\Temp\\ipykernel_5144\\4237983600.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_resampled[oversample_col] = y_resampled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masculino               12709\n",
      "Feminino                12709\n",
      "Prefiro não informar    12709\n",
      "Outro                   12709\n",
      "Name: P1_b, dtype: int64\n",
      "25-29    15468\n",
      "30-34    15468\n",
      "40-44    15468\n",
      "17-21    15468\n",
      "35-39    15468\n",
      "22-24    15468\n",
      "45-49    15468\n",
      "50-54    15468\n",
      "55+      15468\n",
      "Name: P1_a_1, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vi_ma\\AppData\\Local\\Temp\\ipykernel_5144\\4237983600.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_resampled[oversample_col] = y_resampled\n"
     ]
    }
   ],
   "source": [
    "col_raca = 'P1_c'\n",
    "col_genero = 'P1_b'\n",
    "col_idade ='P1_a_1'\n",
    "# col_regiao = 'P1_i_2';\n",
    "\n",
    "cols_resampling = [col_raca, col_genero, col_idade ] # , col_regiao]\n",
    "\n",
    "df_resampled = train.copy()\n",
    "\n",
    "for col in cols_resampling:\n",
    "    df_resampled = apply_random_oversampling(df_resampled, oversample_col=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = [\"P1_m\", \"P3_c\",\"P4_a\",\"P4_b\", \"P4_c\", \"P4_d\", \"P4_e\", \"P4_g\", \"P4_j\", \"P5_b\", \"P6_a\", \"P6_b\", \"P6_g\", \"P6_h\", \"P7_a\", \"P7_b\", \"P7_d\", \"P8_a\", \"P8_b\", \"P8_c\", \"P8_d\", \"P2_b\", \"P2_d\", \"P2_f\", \"P2_o\", \"P2_r\", \"P6_c\", \"P6_d\", \"P6_e\", \"P6_f\"]\n",
    "ordered = [\"P1_l\", \"P2_e\", \"P2_g\", \"P2_i\", \"P2_j\", \"P2_h\"]\n",
    "sensible = ['P1_c', 'P1_b','P1_a_1','P1_i_2']\n",
    "\n",
    "# df_feat_selected = df_resampled[ [x +endswith(\"(OneHot)\") for x in onehot ] + [x +endswith(\"(OrdEnc)\") for x in ordered] ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_cols = [col for col in df_resampled.columns if any(col.startswith(c) for c in onehot) and col.endswith('(OneHot)')]\n",
    "ordered_cols = [col for col in df_resampled.columns if any(col.startswith(c) for c in ordered) and col.endswith('(OrdEnc)')]\n",
    "df_feat_selected = df_resampled[onehot_cols+ordered_cols+sensible]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['P1_m_Ciências Biológicas/ Farmácia/ Medicina/ Área da Saúde(OneHot)',\n",
       "       'P1_m_Ciências Sociais(OneHot)',\n",
       "       'P1_m_Computação / Engenharia de Software / Sistemas de Informação/ TI(OneHot)',\n",
       "       'P1_m_Economia/ Administração / Contabilidade / Finanças/ Negócios(OneHot)',\n",
       "       'P1_m_Estatística/ Matemática / Matemática Computacional/ Ciências Atuariais(OneHot)',\n",
       "       'P1_m_Marketing / Publicidade / Comunicação / Jornalismo(OneHot)',\n",
       "       'P1_m_Outra opção(OneHot)', 'P1_m_Outras Engenharias(OneHot)',\n",
       "       'P1_m_Química / Física(OneHot)', 'P1_m_nan(OneHot)',\n",
       "       ...\n",
       "       '35-39', '40-44', '45-49', '50-54', '55+', 'Centro-oeste', 'Nordeste',\n",
       "       'Norte', 'Sudeste', 'Sul'],\n",
       "      dtype='object', length=122)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sens_col in sensible:\n",
    "    df_feat_selected = pd.concat([df_feat_selected, pd.get_dummies(df_feat_selected[sens_col])], axis=1)\n",
    "\n",
    "df_feat_selected.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feat_selected = test[onehot_cols+ordered_cols+sensible]\n",
    "\n",
    "for sens_col in sensible:\n",
    "    test_feat_selected = pd.concat([test_feat_selected, pd.get_dummies(test_feat_selected[sens_col])], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_feat_selected.drop(columns=['P2_h(OrdEnc)'])\n",
    "y_train = df_feat_selected['P2_h(OrdEnc)']\n",
    "\n",
    "X_test = test_feat_selected.drop(columns=['P2_h(OrdEnc)'])\n",
    "y_test = test_feat_selected['P2_h(OrdEnc)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vi_ma\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "logistic_regression = LogisticRegression(max_iter=1000)\n",
    "logistic_regression.fit(X_train.drop(columns=sensible), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train.drop(columns=sensible), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train.drop(columns=sensible), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vi_ma\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the QDA model with the balanced dataset\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train.drop(columns=sensible), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Indígena'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(X_train.columns)-set(X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vi_ma\\anaconda3\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\vi_ma\\anaconda3\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\vi_ma\\anaconda3\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\vi_ma\\anaconda3\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QDA</td>\n",
       "      <td>0.157729</td>\n",
       "      <td>[0.0, 0.0, 0.06666666666666667, 0.082872928176...</td>\n",
       "      <td>[0.0, 0.0, 0.12698412698412698, 0.227272727272...</td>\n",
       "      <td>[0.0, 0.0, 0.08743169398907104, 0.121457489878...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.249211</td>\n",
       "      <td>[0.0, 0.2191780821917808, 0.16666666666666666,...</td>\n",
       "      <td>[0.0, 0.41025641025641024, 0.19047619047619047...</td>\n",
       "      <td>[0.0, 0.2857142857142857, 0.17777777777777778,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.242902</td>\n",
       "      <td>[0.08333333333333333, 0.1206896551724138, 0.17...</td>\n",
       "      <td>[0.1111111111111111, 0.1794871794871795, 0.190...</td>\n",
       "      <td>[0.09523809523809525, 0.1443298969072165, 0.18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.222923</td>\n",
       "      <td>[0.6666666666666666, 0.2727272727272727, 0.180...</td>\n",
       "      <td>[0.2222222222222222, 0.3076923076923077, 0.206...</td>\n",
       "      <td>[0.3333333333333333, 0.2891566265060241, 0.192...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  \\\n",
       "0                  QDA  0.157729   \n",
       "1  Logistic Regression  0.249211   \n",
       "2                  KNN  0.242902   \n",
       "3        Decision Tree  0.222923   \n",
       "\n",
       "                                           Precision  \\\n",
       "0  [0.0, 0.0, 0.06666666666666667, 0.082872928176...   \n",
       "1  [0.0, 0.2191780821917808, 0.16666666666666666,...   \n",
       "2  [0.08333333333333333, 0.1206896551724138, 0.17...   \n",
       "3  [0.6666666666666666, 0.2727272727272727, 0.180...   \n",
       "\n",
       "                                              Recall  \\\n",
       "0  [0.0, 0.0, 0.12698412698412698, 0.227272727272...   \n",
       "1  [0.0, 0.41025641025641024, 0.19047619047619047...   \n",
       "2  [0.1111111111111111, 0.1794871794871795, 0.190...   \n",
       "3  [0.2222222222222222, 0.3076923076923077, 0.206...   \n",
       "\n",
       "                                            F1-Score  \n",
       "0  [0.0, 0.0, 0.08743169398907104, 0.121457489878...  \n",
       "1  [0.0, 0.2857142857142857, 0.17777777777777778,...  \n",
       "2  [0.09523809523809525, 0.1443298969072165, 0.18...  \n",
       "3  [0.3333333333333333, 0.2891566265060241, 0.192...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Teste\n",
    "# gambiarra porque no teste não tinha nenhuma classe indígena, então a coluna não foi criada no get dummies\n",
    "X_test['Indígena'] = 0\n",
    "\n",
    "#\n",
    "models = {\n",
    "    'QDA': qda,\n",
    "    'Logistic Regression': logistic_regression,\n",
    "    'KNN': knn,\n",
    "    'Decision Tree': decision_tree\n",
    "}\n",
    "\n",
    "results = []\n",
    "for model_name, model in models.items():\n",
    "    y_pred = model.predict(X_test.drop(columns=sensible))\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=None, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average=None, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average=None, zero_division=0)\n",
    "    results.append([model_name, accuracy, precision, recall, f1])\n",
    "\n",
    "# Store the results in a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "\n",
    "# Print the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUNA P1_c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vi_ma\\anaconda3\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [951, 222]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m X_test[sens_col]\u001b[38;5;241m.\u001b[39munique():\n\u001b[0;32m      7\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test[X_test[sens_col]\u001b[38;5;241m==\u001b[39mc]\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39msensible))\n\u001b[1;32m----> 8\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m f1_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClasse \u001b[39m\u001b[38;5;132;01m{c}\u001b[39;00m\u001b[38;5;124m: acc=\u001b[39m\u001b[38;5;132;01m{accuraccy}\u001b[39;00m\u001b[38;5;124m / f1=\u001b[39m\u001b[38;5;132;01m{f1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vi_ma\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:211\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\vi_ma\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true)\n\u001b[0;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred)\n",
      "File \u001b[1;32mc:\\Users\\vi_ma\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    330\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    335\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [951, 222]"
     ]
    }
   ],
   "source": [
    "# Teste POR CLASSE SENSÍVEL\n",
    "model =  logistic_regression\n",
    "\n",
    "for sens_col in sensible:\n",
    "    print(f\"COLUNA {sens_col}\")\n",
    "    for c in X_test[sens_col].unique():\n",
    "        y_pred = model.predict(X_test[X_test[sens_col]==c].drop(columns=sensible))\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        print(\"Classe {c}: acc={accuraccy} / f1={f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
