{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import rich\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset_path = Path('../data/State_of_data_BR_2023_Kaggle - df_survey_2023.csv')\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">540</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m540\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count values of nan in ('P2_h ', 'Faixa salarial')\n",
    "rich.print(df[\"('P2_h ', 'Faixa salarial')\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path: Path) -> dict:\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "def save_json(file_path: Path, data: dict) -> None:\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, ensure_ascii=False ,indent=4)\n",
    "\n",
    "# Load the merged question\n",
    "merged_json_path = Path(\"../questions_jsons/completions_good_features.json\")\n",
    "questions_merged_dict = load_json(merged_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('P6_b_16 ', 'SQL Server Integration Services (SSIS))\n"
     ]
    }
   ],
   "source": [
    "# criando um dicionário com o número da pergunta e qual é a pergunta\n",
    "column_dict = {}\n",
    "\n",
    "for col in df.columns:\n",
    "\n",
    "    match = re.match(r\"\\('(.+?)', '(.+?)'\\)\", col)\n",
    "    if match:\n",
    "        cod, pergunta = match.groups()\n",
    "        column_dict[cod.replace(\" \", \"\")] = pergunta\n",
    "    else:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_dict['P6_b_16']='SQL Server Integration Services (SSIS)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treat = df.copy()\n",
    "df_treat.columns = list(column_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually fix some columns\n",
    "\n",
    "# rename wrong column\n",
    "df_treat.rename(columns={'P7_1': 'P7_a'}, inplace=True)\n",
    "df_treat.rename(columns={'P8_3': 'P8_c'}, inplace=True)\n",
    "\n",
    "# Switch two columns that were in the wrong order\n",
    "df_treat.rename(columns={'P7_a': 'P7_a_1', 'P7_a_1': 'P7_a'}, inplace=True) # P7_a_1 (antiga P7_a), has corrupted data\n",
    "df_treat.rename(columns={'P8_b': 'P8_b_1', 'P8_b_1': 'P8_b'}, inplace=True) \n",
    "df_treat.rename(columns={'P8_c': 'P8_c_1', 'P8_c_1': 'P8_c'}, inplace=True) \n",
    "df_treat.rename(columns={'P4_a_1': 'P4_a2'}, inplace=True) # P4_a_1 Is not an alternative for P4_a \n",
    "\n",
    "# Remove columns\n",
    "#df_treat.drop(columns=[''], inplace=True) \n",
    "\n",
    "# Create new dummy columns with 0 on all rows\n",
    "#df_treat[''] = 0\n",
    "\n",
    "# Add columns to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*Ciência de Dados/Machine Learning/AI: *Desenha e executa experimentos com o objetivo de responder perguntas do negócio; desenvolve modelos preditivos e algoritmos de Machine Learning com o objetivo de otimizar e automatizar a tomada de decisão.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column = \"P4_a\"\n",
    "df_treat_not_null = df_treat[df_treat[column].notnull()]\n",
    "df_treat_not_null[column].iloc[0]\n",
    "\n",
    "# df_treat_not_null[[\"P7_a\",\"P7_a_1\",\"P7_a_2\",\"P7_a_3\",\"P7_a_4\",\"P7_a_5\",\"P7_a_6\",\"P7_a_7\",\"P7_a_8\",\"P7_a_9\",\"P7_a_10\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Categorical columns: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'P1_l'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P1_m'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_e'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_f'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_g'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_i'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_j'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_o'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_r'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P3_c'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'P4_a'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P4_b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P4_c'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P4_d'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P4_e'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P4_g'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P4_j'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P5_b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P6_a'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P6_b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P6_e'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P6_g'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P6_h'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P7_a'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'P8_b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P8_c'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Categorical columns: \u001b[1m[\u001b[0m\u001b[32m'P1_l'\u001b[0m, \u001b[32m'P1_m'\u001b[0m, \u001b[32m'P2_b'\u001b[0m, \u001b[32m'P2_e'\u001b[0m, \u001b[32m'P2_f'\u001b[0m, \u001b[32m'P2_g'\u001b[0m, \u001b[32m'P2_i'\u001b[0m, \u001b[32m'P2_j'\u001b[0m, \u001b[32m'P2_o'\u001b[0m, \u001b[32m'P2_r'\u001b[0m, \u001b[32m'P3_c'\u001b[0m, \n",
       "\u001b[32m'P4_a'\u001b[0m, \u001b[32m'P4_b'\u001b[0m, \u001b[32m'P4_c'\u001b[0m, \u001b[32m'P4_d'\u001b[0m, \u001b[32m'P4_e'\u001b[0m, \u001b[32m'P4_g'\u001b[0m, \u001b[32m'P4_j'\u001b[0m, \u001b[32m'P5_b'\u001b[0m, \u001b[32m'P6_a'\u001b[0m, \u001b[32m'P6_b'\u001b[0m, \u001b[32m'P6_e'\u001b[0m, \u001b[32m'P6_g'\u001b[0m, \u001b[32m'P6_h'\u001b[0m, \u001b[32m'P7_a'\u001b[0m, \n",
       "\u001b[32m'P8_b'\u001b[0m, \u001b[32m'P8_c'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Categorical columns with alternatives: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'P2_o'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P3_c'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P4_a'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P4_b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P4_c'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P4_d'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P4_g'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P4_j'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P6_a'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'P6_b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P6_h'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P7_a'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P8_b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P8_c'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Categorical columns with alternatives: \u001b[1m[\u001b[0m\u001b[32m'P2_o'\u001b[0m, \u001b[32m'P3_c'\u001b[0m, \u001b[32m'P4_a'\u001b[0m, \u001b[32m'P4_b'\u001b[0m, \u001b[32m'P4_c'\u001b[0m, \u001b[32m'P4_d'\u001b[0m, \u001b[32m'P4_g'\u001b[0m, \u001b[32m'P4_j'\u001b[0m, \u001b[32m'P6_a'\u001b[0m, \n",
       "\u001b[32m'P6_b'\u001b[0m, \u001b[32m'P6_h'\u001b[0m, \u001b[32m'P7_a'\u001b[0m, \u001b[32m'P8_b'\u001b[0m, \u001b[32m'P8_c'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Categorical columns without alternatives: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'P1_l'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P1_m'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_e'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_f'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_g'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_i'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_j'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_r'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'P4_e'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P5_b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P6_e'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P6_g'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Categorical columns without alternatives: \u001b[1m[\u001b[0m\u001b[32m'P1_l'\u001b[0m, \u001b[32m'P1_m'\u001b[0m, \u001b[32m'P2_b'\u001b[0m, \u001b[32m'P2_e'\u001b[0m, \u001b[32m'P2_f'\u001b[0m, \u001b[32m'P2_g'\u001b[0m, \u001b[32m'P2_i'\u001b[0m, \u001b[32m'P2_j'\u001b[0m, \u001b[32m'P2_r'\u001b[0m, \n",
       "\u001b[32m'P4_e'\u001b[0m, \u001b[32m'P5_b'\u001b[0m, \u001b[32m'P6_e'\u001b[0m, \u001b[32m'P6_g'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Numerical columns: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'P2_d'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P6_c'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P6_d'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P6_f'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P7_b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P7_d'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P8_a'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P8_d'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Numerical columns: \u001b[1m[\u001b[0m\u001b[32m'P2_d'\u001b[0m, \u001b[32m'P6_c'\u001b[0m, \u001b[32m'P6_d'\u001b[0m, \u001b[32m'P6_f'\u001b[0m, \u001b[32m'P7_b'\u001b[0m, \u001b[32m'P7_d'\u001b[0m, \u001b[32m'P8_a'\u001b[0m, \u001b[32m'P8_d'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Possible columns for ordering: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'P1_l'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_e'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_g'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_h'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_i'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'P2_j'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Possible columns for ordering: \u001b[1m[\u001b[0m\u001b[32m'P1_l'\u001b[0m, \u001b[32m'P2_e'\u001b[0m, \u001b[32m'P2_g'\u001b[0m, \u001b[32m'P2_h'\u001b[0m, \u001b[32m'P2_i'\u001b[0m, \u001b[32m'P2_j'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Manual Analysis\n",
    "columns_to_maintain = [\"P1_l\", \"P1_m\", \"P2_i\", \"P2_j\", \"P3_c\",\n",
    "                    \"P4_a\", # Mas não a P4_a_1\n",
    "                \"P4_b\", \"P4_c\", \"P4_d\", \"P4_e\", \"P4_g\", \"P4_j\", \"P5_b\", \"P6_a\", \"P6_b\", \"P6_g\", \"P6_h\", \"P7_a\", \"P7_b\", \"P7_d\", \"P8_a\", \"P8_b\", \"P8_c\", \"P8_d\"\n",
    "                    ]\n",
    "\n",
    "column_in_doubt = [\n",
    "    \"P2_b\", \"P2_d\", \"P2_e\", \"P2_f\", \"P2_g\", # Dados de emprego atuais da pessoa (cargo, se é gerente, setor, etc.)\n",
    "    \"P2_o\", # Critérios que a pessoa acha importante ao escolher uma vaga de emprego (salário, clima, desenvolvimento, etc.)\n",
    "    \"P2_r\", # Modelo de trabalho atual da pessoa (remoto, presencial, híbrido). Estou mais para o não\n",
    "    \"P6_c\", \"P6_d\", \"P6_e\", \"P6_f\" # Se a empresa que trabalha tem data lake, data warehouse, etc.\n",
    "    ]\n",
    "\n",
    "possible_for_ordering = [\"P1_l\", \"P2_e\", \"P2_g\", \"P2_i\", \"P2_j\"] # \"P2_r\"]\n",
    "\n",
    "target = \"P2_h\"\n",
    "\n",
    "possible_for_ordering = possible_for_ordering + [target]\n",
    "\n",
    "# For now, use both columns_to_maintain and column_in_doubt\n",
    "# categorical columns and numerical columns\n",
    "# colunas categóricas\n",
    "cat_cols = [col for col in df_treat.columns if df_treat[col].dtype=='O' and col in (columns_to_maintain + column_in_doubt)]\n",
    "\n",
    "# colunas numéricas\n",
    "num_cols = [col for col in df_treat.columns if (df_treat[col].dtype=='int64' or df_treat[col].dtype=='float64') and col in (columns_to_maintain + column_in_doubt)]\n",
    "\n",
    "merged_questions = load_json(Path(\"../questions_jsons/merged_questions.json\"))\n",
    "# Flatten the merged_questions dictionary to remove part divisions\n",
    "merged_questions_no_part_division = {\n",
    "    question['col_id']: question\n",
    "    for part in merged_questions.values()\n",
    "    for question in part\n",
    "}\n",
    "\n",
    "\n",
    "cat_cols_with_alternatives = [cat_col for cat_col in cat_cols if \"possiveis_respostas\" in merged_questions_no_part_division[cat_col]]\n",
    "\n",
    "cat_cols_without_alternatives = sorted((set(cat_cols) - set(cat_cols_with_alternatives)))\n",
    "\n",
    "rich.print(f\"Categorical columns: {sorted(cat_cols)}\")\n",
    "rich.print(f\"Categorical columns with alternatives: {sorted(cat_cols_with_alternatives)}\")\n",
    "rich.print(f\"Categorical columns without alternatives: {cat_cols_without_alternatives}\")\n",
    "rich.print(f\"Numerical columns: {sorted(num_cols)}\")\n",
    "rich.print(f\"Possible columns for ordering: {sorted(possible_for_ordering)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treat columns for later encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:09<00:00,  1.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# Wrapper class for alternatives, handles creation and normalization\n",
    "class AlternativeWrapper:\n",
    "    def __init__(self, alternative_name, alternative_answer):\n",
    "        # Store original answer and normalized form\n",
    "        self.alternative_name = alternative_name\n",
    "        self.alternative_answer = alternative_answer\n",
    "        self.alternative_answer_normalized = self._normalize_answer(alternative_answer)\n",
    "        self.alternative_name_from_list = alternative_name + \"(FromList)\"\n",
    "\n",
    "    def _normalize_answer(self, answer) -> str:\n",
    "        # Normalizes the answer to lowercase and removes special characters\n",
    "        return re.sub(r\"[^0-9a-záàâãéèêíïóôõöúçñ\\n +]\", \" \", answer.lower()).rstrip()\n",
    "\n",
    "    def create_column_for_alternative(self, df):\n",
    "        # Adds a column for each alternative with a default value of 0\n",
    "        df[self.alternative_name_from_list] = 0\n",
    "\n",
    "    def set_column_from_list_to_1(self, df, index):\n",
    "        # Set the column value to 1 if the alternative is selected\n",
    "        df.at[index, self.alternative_name_from_list] = 1\n",
    "\n",
    "def deal_with_special_cases(alternatives: list[AlternativeWrapper]):\n",
    "    \"\"\"\n",
    "    Deals with special cases where the alternatives have some error on them.\n",
    "    \"\"\"\n",
    "    # On P8_b_8, the alternative uses the word \"desenvolvo\", but the final responses collected uses \"utilizo\"\n",
    "    for alternative in alternatives:\n",
    "        if alternative.alternative_name == \"P8_b_8\":\n",
    "            alternative.alternative_answer_normalized = alternative.alternative_answer_normalized.replace(\"desenvolvo\", \"utilizo\")\n",
    "    return alternatives\n",
    "\n",
    "def create_alternative_wrappers_and_columns(alternatives: dict, df: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    Creates a list of AlternativeWrapper objects and adds corresponding columns to the DataFrame.\n",
    "    \"\"\"\n",
    "    alternative_wrappers = []\n",
    "    for alternative_name, alternative_answer in alternatives.items():\n",
    "        wrapper = AlternativeWrapper(alternative_name, alternative_answer)\n",
    "        wrapper.create_column_for_alternative(df)\n",
    "        alternative_wrappers.append(wrapper)\n",
    "    return alternative_wrappers\n",
    "\n",
    "def normalize_string(input_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes a string by removing special characters and converting to lowercase.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"[^0-9a-záàâãéèêíïóôõöúçñ\\n+]\", \" \", input_str.lower())\n",
    "\n",
    "def process_row_alternatives(df: pd.DataFrame, index: int, row: pd.Series, col: str, alternatives: list, debug: bool = False):\n",
    "    \"\"\"\n",
    "    Processes a single row to find matching alternatives and update the corresponding columns.\n",
    "    \"\"\"\n",
    "    if pd.notna(row[col]):\n",
    "        alternatives = deal_with_special_cases(alternatives)\n",
    "        \n",
    "        # Normalize the row answer and find alternatives in it\n",
    "        normalized_answer_str = normalize_string(row[col])\n",
    "        normalized_alternatives = [alt.alternative_answer_normalized for alt in alternatives]\n",
    "\n",
    "        \n",
    "        # Find matches between normalized answers and alternatives\n",
    "        matched_answers = [alt for alt in normalized_alternatives if alt.replace(\" \", \"\") in normalized_answer_str.replace(\" \", \"\")]\n",
    "        df.at[index, col + \"(List)\"] = matched_answers\n",
    "\n",
    "        # Set the corresponding alternative columns to 1 if matched\n",
    "        for matched_answer in matched_answers:\n",
    "            for alternative in alternatives:\n",
    "                if alternative.alternative_answer_normalized == matched_answer:\n",
    "                    alternative.set_column_from_list_to_1(df, index)\n",
    "\n",
    "        handle_unmatched_answers(df, index, col, normalized_answer_str, normalized_alternatives, debug)\n",
    "\n",
    "def handle_unmatched_answers(df: pd.DataFrame, index: int, col: str, normalized_answers_str: str, normalized_alternatives: str, debug: bool = False):\n",
    "    \"\"\"\n",
    "    Handles situations where there are unmatched answers in the row.\n",
    "    \"\"\"\n",
    "    list_of_answer_str = \"  \".join(df.at[index, col + \"(List)\"])\n",
    "    normalized_alternatives_str = \"  \".join(normalized_alternatives) \n",
    "    \n",
    "    # Check if all answers are found by comparing string lengths\n",
    "    if len(normalized_answers_str) != len(list_of_answer_str):\n",
    "        if len(list_of_answer_str) == 0:\n",
    "            # Likely an \"outros\" option, set list to empty or consider dropping the row\n",
    "            df.at[index, col + \"(List)\"] = []\n",
    "            # Optionally: df.drop(index, inplace=True)\n",
    "        if debug:\n",
    "            if abs(len(normalized_answers_str) - len(list_of_answer_str)) > 200:\n",
    "                rich.print(f\"col {col} at index {index}\")\n",
    "                rich.print(f\"Answers found: {df.at[index, col + '(List)']}\")\n",
    "                rich.print(f\"Original answers normalized: {normalized_answers_str}\")\n",
    "                rich.print(f\"Alternatives normalized: {normalized_alternatives_str}\")\n",
    "                rich.print(len(normalized_answers_str), \" | \", len(list_of_answer_str))\n",
    "\n",
    "def process_dataframe(df: pd.DataFrame, cat_cols_with_alternatives: list, merged_questions_no_part_division: dict, debug: bool = False):\n",
    "    \"\"\"\n",
    "    Main function to process the DataFrame, adding new columns and handling alternative answers.\n",
    "    \"\"\"\n",
    "    for col in tqdm(cat_cols_with_alternatives):\n",
    "        df[col + \"(List)\"] = [[] for _ in range(len(df))]\n",
    "\n",
    "        # Get the possible alternatives for the column and create their wrappers\n",
    "        alternatives = create_alternative_wrappers_and_columns(\n",
    "            merged_questions_no_part_division[col][\"possiveis_respostas\"], df\n",
    "        )\n",
    "\n",
    "        # Process each row in the DataFrame for the current column\n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                process_row_alternatives(df, index, row, col, alternatives, debug)\n",
    "            except Exception as e:\n",
    "                rich.print(f\"Error on column {col} at index {index}\")\n",
    "                rich.print(f\"Value being set: {df.at[index, col + '(List)']}\")\n",
    "                raise e\n",
    "\n",
    "process_dataframe(df_treat, cat_cols_with_alternatives, merged_questions_no_part_division, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the treated dataframe\n",
    "df_treat.to_csv(\"../data/treated_data_regex.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_treat.drop(columns=[target])\n",
    "y = df_treat[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_concat = pd.concat([X_train, y_train], axis=1)\n",
    "test_concat = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save concated dataframes\n",
    "train_concat.to_csv(\"../data/train_concat_regex.csv\", index=False)\n",
    "test_concat.to_csv(\"../data/test_concat_regex.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Columns\n",
    "\n",
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load split datasets \n",
    "train_concat = pd.read_csv(\"../data/train_concat_regex.csv\")\n",
    "test_concat = pd.read_csv(\"../data/test_concat_regex.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING DATA\n",
    "\n",
    "# Transform the columns that do not have alternatives into one hot encoding\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist').set_output(transform=\"pandas\")\n",
    "\n",
    "ohetransform = ohe.fit_transform(train_concat[cat_cols_without_alternatives])\n",
    "\n",
    "# Rename all columns to have an (OneHot) identifier\n",
    "ohetransform.columns = [col + \"(OneHot)\" for col in ohetransform.columns]\n",
    "\n",
    "# Merge the one hot encoding columns with the original dataframe\n",
    "train_concat = pd.concat([train_concat, ohetransform], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1_l_Doutorado ou Phd(OneHot)</th>\n",
       "      <th>P1_l_Estudante de Graduação(OneHot)</th>\n",
       "      <th>P1_l_Graduação/Bacharelado(OneHot)</th>\n",
       "      <th>P1_l_Mestrado(OneHot)</th>\n",
       "      <th>P1_l_Não tenho graduação formal(OneHot)</th>\n",
       "      <th>P1_l_Prefiro não informar(OneHot)</th>\n",
       "      <th>P1_l_Pós-graduação(OneHot)</th>\n",
       "      <th>P1_m_Ciências Biológicas/ Farmácia/ Medicina/ Área da Saúde(OneHot)</th>\n",
       "      <th>P1_m_Ciências Sociais(OneHot)</th>\n",
       "      <th>P1_m_Computação / Engenharia de Software / Sistemas de Informação/ TI(OneHot)</th>\n",
       "      <th>...</th>\n",
       "      <th>P6_g_Azure(OneHot)</th>\n",
       "      <th>P6_g_Databricks(OneHot)</th>\n",
       "      <th>P6_g_Google BigQuery(OneHot)</th>\n",
       "      <th>P6_g_IBM(OneHot)</th>\n",
       "      <th>P6_g_Oracle(OneHot)</th>\n",
       "      <th>P6_g_Postgres/MySQL(OneHot)</th>\n",
       "      <th>P6_g_Presto(OneHot)</th>\n",
       "      <th>P6_g_Snowflake(OneHot)</th>\n",
       "      <th>P6_g_Teradata(OneHot)</th>\n",
       "      <th>P6_g_nan(OneHot)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   P1_l_Doutorado ou Phd(OneHot)  P1_l_Estudante de Graduação(OneHot)  \\\n",
       "0                            0.0                                  0.0   \n",
       "\n",
       "   P1_l_Graduação/Bacharelado(OneHot)  P1_l_Mestrado(OneHot)  \\\n",
       "0                                 1.0                    0.0   \n",
       "\n",
       "   P1_l_Não tenho graduação formal(OneHot)  P1_l_Prefiro não informar(OneHot)  \\\n",
       "0                                      0.0                                0.0   \n",
       "\n",
       "   P1_l_Pós-graduação(OneHot)  \\\n",
       "0                         0.0   \n",
       "\n",
       "   P1_m_Ciências Biológicas/ Farmácia/ Medicina/ Área da Saúde(OneHot)  \\\n",
       "0                                                0.0                     \n",
       "\n",
       "   P1_m_Ciências Sociais(OneHot)  \\\n",
       "0                            0.0   \n",
       "\n",
       "   P1_m_Computação / Engenharia de Software / Sistemas de Informação/ TI(OneHot)  \\\n",
       "0                                                0.0                               \n",
       "\n",
       "   ...  P6_g_Azure(OneHot)  P6_g_Databricks(OneHot)  \\\n",
       "0  ...                 0.0                      0.0   \n",
       "\n",
       "   P6_g_Google BigQuery(OneHot)  P6_g_IBM(OneHot)  P6_g_Oracle(OneHot)  \\\n",
       "0                           0.0               0.0                  0.0   \n",
       "\n",
       "   P6_g_Postgres/MySQL(OneHot)  P6_g_Presto(OneHot)  P6_g_Snowflake(OneHot)  \\\n",
       "0                          0.0                  0.0                     0.0   \n",
       "\n",
       "   P6_g_Teradata(OneHot)  P6_g_nan(OneHot)  \n",
       "0                    0.0               1.0  \n",
       "\n",
       "[1 rows x 135 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_concat[[col for col in train_concat.columns if \"(OneHot)\" in col]].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DATA\n",
    "\n",
    "# Transform the columns that do not have alternatives into one hot encoding\n",
    "\n",
    "ohetransform = ohe.transform(test_concat[cat_cols_without_alternatives])\n",
    "\n",
    "# Rename all columns to have an (OneHot) identifier\n",
    "ohetransform.columns = [col + \"(OneHot)\" for col in ohetransform.columns]\n",
    "\n",
    "# Merge the one hot encoding columns with the original dataframe\n",
    "test_concat = pd.concat([test_concat, ohetransform], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P2_r</th>\n",
       "      <th>P2_r_Modelo 100% presencial(OneHot)</th>\n",
       "      <th>P2_r_Modelo 100% remoto(OneHot)</th>\n",
       "      <th>P2_r_Modelo híbrido com dias fixos de trabalho presencial(OneHot)</th>\n",
       "      <th>P2_r_Modelo híbrido flexível (o funcionário tem liberdade para escolher quando estar no escritório presencialmente)(OneHot)</th>\n",
       "      <th>P2_r_nan(OneHot)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Modelo 100% remoto</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 P2_r  P2_r_Modelo 100% presencial(OneHot)  \\\n",
       "0  Modelo 100% remoto                                  0.0   \n",
       "\n",
       "   P2_r_Modelo 100% remoto(OneHot)  \\\n",
       "0                              1.0   \n",
       "\n",
       "   P2_r_Modelo híbrido com dias fixos de trabalho presencial(OneHot)  \\\n",
       "0                                                0.0                   \n",
       "\n",
       "   P2_r_Modelo híbrido flexível (o funcionário tem liberdade para escolher quando estar no escritório presencialmente)(OneHot)  \\\n",
       "0                                                0.0                                                                             \n",
       "\n",
       "   P2_r_nan(OneHot)  \n",
       "0               0.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_concat[[col for col in test_concat.columns if \"P2_r\" in col]].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Cientista de Dados/Data Scientist', 'Analista de BI/BI Analyst',\n",
       "       'Analista de Dados/Data Analyst', nan,\n",
       "       'Engenheiro de Dados/Arquiteto de Dados/Data Engineer/Data Architect',\n",
       "       'Analista de Negócios/Business Analyst', 'Outra Opção',\n",
       "       'Desenvolvedor/ Engenheiro de Software/ Analista de Sistemas',\n",
       "       'Professor/Pesquisador', 'Analytics Engineer', 'Economista',\n",
       "       'Estatístico', 'Analista de Suporte/Analista Técnico',\n",
       "       'Data Product Manager/ Product Manager (PM/APM/DPM/GPM/PO)',\n",
       "       'Outras Engenharias (não inclui dev)',\n",
       "       'Analista de Inteligência de Mercado/Market Intelligence',\n",
       "       'Engenheiro de Machine Learning/ML Engineer/AI Engineer',\n",
       "       'DBA/Administrador de Banco de Dados'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_treat[\"P2_f\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4234 entries, 0 to 4233\n",
      "Columns: 734 entries, P0 to P6_g_nan(OneHot)\n",
      "dtypes: float64(463), int64(188), object(83)\n",
      "memory usage: 23.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len Binary Columns: 650\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4234 entries, 0 to 4233\n",
      "Columns: 734 entries, P0 to P6_g_nan(OneHot)\n",
      "dtypes: bool(650), int16(1), object(83)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Before doing the ordinal encoding, reduce the memory usage of the dataframe by transforming every numerical column to a binary column\n",
    "\n",
    "binary_cols = [col for col in train_concat.columns if train_concat[col].dropna().isin([0, 1, 0.0, 1.0]).all()]\n",
    "# The len for the binary columns is 652, and the total of float and int types is 653. The missing column is P1_a, which has the age of the person.\n",
    "print(\"Len Binary Columns: \" + str(len(binary_cols)))\n",
    "\n",
    "# Convert binary columns to bool type\n",
    "train_concat[binary_cols] = train_concat[binary_cols].astype(bool)\n",
    "\n",
    "# For the age of a person, int16 is enough \n",
    "train_concat['P1_a'] = train_concat['P1_a'].astype('int16')\n",
    "\n",
    "train_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len Binary Columns: 650\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1059 entries, 0 to 1058\n",
      "Columns: 734 entries, P0 to P6_g_nan(OneHot)\n",
      "dtypes: bool(650), int16(1), object(83)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Before doing the ordinal encoding, reduce the memory usage of the dataframe by transforming every numerical column to a binary column\n",
    "\n",
    "binary_cols = [col for col in test_concat.columns if test_concat[col].dropna().isin([0, 1, 0.0, 1.0]).all()]\n",
    "# The len for the binary columns is 652, and the total of float and int types is 653. The missing column is P1_a, which has the age of the person.\n",
    "print(\"Len Binary Columns: \" + str(len(binary_cols)))\n",
    "\n",
    "# Convert binary columns to bool type\n",
    "test_concat[binary_cols] = test_concat[binary_cols].astype(bool)\n",
    "\n",
    "# For the age of a person, int16 is enough \n",
    "test_concat['P1_a'] = test_concat['P1_a'].astype('int16')\n",
    "\n",
    "test_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with \"de R$ 101/mês a R$ 2.000/mês\" on P2_h\n",
    "train_concat = train_concat[train_concat[\"P2_h\"] != \"de R$ 101/mês a R$ 2.000/mês\"]\n",
    "test_concat = test_concat[test_concat[\"P2_h\"] != \"de R$ 101/mês a R$ 2.000/mês\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "\n",
    "# Transform the columns possible for ordering into ordinal encoding\n",
    "\n",
    "# Possible columns for ordering: [\"P1_l\", \"P2_e\", \"P2_g\", \"P2_i\", \"P2_j\", \"P2_r\"]\n",
    "\n",
    "categories = {\n",
    "    \"P1_l\": [\"Prefiro não informar\", \"Não tenho graduação formal\", \"Estudante de Graduação\", \"Graduação/Bacharelado\", \"Pós-graduação\", \"Mestrado\", \"Doutorado ou Phd\"],\n",
    "    \"P2_e\": [\"Team Leader/Tech Leader\", \"Supervisor/Coordenador\", \"Gerente/Head\", \"Diretor/VP\", \"Sócio ou C-level (CEO, CDO, CIO, CTO etc)\"],\n",
    "    \"P2_g\": [\"Júnior\", \"Pleno\", \"Sênior\"],\n",
    "    \"P2_i\": [\"Não tenho experiência na área de dados\", \"Menos de 1 ano\", \"de 1 a 2 anos\", \"de 3 a 4 anos\", \"de 4 a 6 anos\", \"de 5 a 6 anos\", \"de 7 a 10 anos\", \"Mais de 10 anos\"],\n",
    "    \"P2_j\": [\"Não tive experiência na área de TI/Engenharia de Software antes de começar a trabalhar na área de dados\", \"Menos de 1 ano\", \"de 1 a 2 anos\", \"de 3 a 4 anos\", \"de 5 a 6 anos\", \"de 7 a 10 anos\", \"Mais de 10 anos\"],\n",
    "    \"P2_h\": [\n",
    "        \"Menos de R$ 1.000/mês\",\n",
    "        \"de R$ 1.001/mês a R$ 2.000/mês\",\n",
    "        \"de R$ 2.001/mês a R$ 3.000/mês\",\n",
    "        \"de R$ 3.001/mês a R$ 4.000/mês\",\n",
    "        \"de R$ 4.001/mês a R$ 6.000/mês\",\n",
    "        \"de R$ 6.001/mês a R$ 8.000/mês\",\n",
    "        \"de R$ 8.001/mês a R$ 12.000/mês\",\n",
    "        \"de R$ 12.001/mês a R$ 16.000/mês\",\n",
    "        \"de R$ 16.001/mês a R$ 20.000/mês\",\n",
    "        \"de R$ 20.001/mês a R$ 25.000/mês\",\n",
    "        \"de R$ 25.001/mês a R$ 30.000/mês\",\n",
    "        \"de R$ 30.001/mês a R$ 40.000/mês\",\n",
    "        \"Acima de R$ 40.001/mês\",\n",
    "        ]\n",
    "    # P2_r\": [\"Modelo 100% remoto\", \"Modelo híbrido com dias fixos de trabalho presencial\", \"Modelo 100% presencial\"]\n",
    "}\n",
    "\n",
    "# Create the ordinal encoder\n",
    "ordinal_encoder = OrdinalEncoder(categories=[categories[col] for col in possible_for_ordering], dtype=np.int8, unknown_value=-1, handle_unknown=\"use_encoded_value\").set_output(transform=\"pandas\")\n",
    "\n",
    "# Fit and transform the columns\n",
    "ordinal_encoder_transform = ordinal_encoder.fit_transform(train_concat[possible_for_ordering])\n",
    "\n",
    "# Rename all columns to have an (OrdEnc) identifier\n",
    "ordinal_encoder_transform.columns = [col + \"(OrdEnc)\" for col in ordinal_encoder_transform.columns]\n",
    "\n",
    "# Merge the one hot encoding columns with the original dataframe\n",
    "train_concat = pd.concat([train_concat, ordinal_encoder_transform], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1_l(OrdEnc)</th>\n",
       "      <th>P2_e(OrdEnc)</th>\n",
       "      <th>P2_g(OrdEnc)</th>\n",
       "      <th>P2_i(OrdEnc)</th>\n",
       "      <th>P2_j(OrdEnc)</th>\n",
       "      <th>P2_h(OrdEnc)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   P1_l(OrdEnc)  P2_e(OrdEnc)  P2_g(OrdEnc)  P2_i(OrdEnc)  P2_j(OrdEnc)  \\\n",
       "0             3            -1             1             2             3   \n",
       "\n",
       "   P2_h(OrdEnc)  \n",
       "0             4  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_concat[[col for col in train_concat.columns if \"(OrdEnc)\" in col]].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "# Fit and transform the columns\n",
    "ordinal_encoder_transform = ordinal_encoder.transform(test_concat[possible_for_ordering])\n",
    "\n",
    "# Rename all columns to have an (OrdEnc) identifier\n",
    "ordinal_encoder_transform.columns = [col + \"(OrdEnc)\" for col in ordinal_encoder_transform.columns]\n",
    "\n",
    "# Merge the one hot encoding columns with the original dataframe\n",
    "test_concat = pd.concat([test_concat, ordinal_encoder_transform], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1_l(OrdEnc)</th>\n",
       "      <th>P2_e(OrdEnc)</th>\n",
       "      <th>P2_g(OrdEnc)</th>\n",
       "      <th>P2_i(OrdEnc)</th>\n",
       "      <th>P2_j(OrdEnc)</th>\n",
       "      <th>P2_h(OrdEnc)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   P1_l(OrdEnc)  P2_e(OrdEnc)  P2_g(OrdEnc)  P2_i(OrdEnc)  P2_j(OrdEnc)  \\\n",
       "0             3            -1             0             2             0   \n",
       "\n",
       "   P2_h(OrdEnc)  \n",
       "0             4  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_concat[[col for col in test_concat.columns if \"(OrdEnc)\" in col]].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  6,  8,  2,  7,  3,  5,  1, -1, 10,  9, 11, 12,  0], dtype=int8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_concat[\"P2_h(OrdEnc)\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the treated dataframe\n",
    "train_concat.to_csv(\"../data/train_complete.csv\", index=False)\n",
    "test_concat.to_csv(\"../data/test_complete.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
